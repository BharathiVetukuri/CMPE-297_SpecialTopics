# ğŸ¤– Reinforcement Learning with GRPO â€” Colab 4

## ğŸ“˜ Overview
This Colab demonstrates **Reinforcement Learning (RL)** using the **GRPO (Generalized Reward Policy Optimization)** framework with **Unsloth.ai**.  

In this setup, the dataset consists of **problems (prompts)** and **answers generated by an LLM reasoning model**.  
The model learns from **self-generated reasoning traces** through **reward feedback**, without needing explicit â€œpreferredâ€ or â€œrejectedâ€ human labels.

---


## âœ… Artifacts

**ğŸ“˜ Colab Notebook:** https://colab.research.google.com/drive/1RT7IpugQXyX7c0fg92y2qjwtFRv9TM6c?usp=sharing 

**ğŸ“¹ Demo Video:**

---

## ğŸ¯ Objective
- Implement **reasoning-based reinforcement learning** using GRPO.  
- Train an open-weight model such as **Qwen3 (4B)** or any Unsloth-compatible LLM.  
- Demonstrate how a model improves by generating its own answers and being optimized via rewards.  
- Record a short **YouTube walkthrough** showing setup, training, and sample outputs.

---

## ğŸ§  Key Concepts
| Concept | Description |
|----------|-------------|
| **GRPO** | Generalized Reward Policy Optimization â€” a simplified PPO variant tailored for reasoning tasks. |
| **Reasoning model** | The LLM generates reasoning steps and final answers for given problems. |
| **Self-improvement loop** | The modelâ€™s outputs are evaluated by a reward model and optimized accordingly. |
| **No explicit human preference data** | Unlike RLHF or DPO, GRPO relies on auto-generated reasoning traces. |

---

## ğŸ› ï¸ Setup Summary

**1. Install Unsloth & Dependencies**

      pip install unsloth torch accelerate transformers trl datasets bitsandbytes
   
**2. Load Model (Example: Qwen3-4B)**

     from unsloth import FastLanguageModel
     model, tokenizer = FastLanguageModel.from_pretrained(
         "unsloth/Qwen3-4B",
         load_in_4bit=True,
         device_map="auto"
     )

**3.C onfigure GRPO Training**

* Define reward functions and reasoning dataset (problems + answers).

* Optimize policy using the GRPO trainer.

**4. Train**

     from unsloth.trainers import GRPOTrainer
     trainer = GRPOTrainer(model=model, tokenizer=tokenizer, dataset=train_dataset)
     trainer.train()

**5. Evaluate**

* Compare reasoning quality and reward scores before vs. after training.

## ğŸ“Š Results Summary

| Aspect | Observation |
|--------|--------------|
| **Reasoning depth** | Model produces more structured and step-by-step explanations |
| **Reward signal** | Increases steadily as model learns reasoning style |
| **Output quality** | Answers become logically coherent and self-consistent |
| **Efficiency** | GRPO converges faster than full PPO implementations |

---

## Screenshots

<img width="860" height="536" alt="image" src="https://github.com/user-attachments/assets/d171153b-f7a2-4ded-a7c7-3a18ff417988" />

<img width="860" height="536" alt="image" src="https://github.com/user-attachments/assets/220a4e62-c8d3-4c94-8aa5-5dc3f2189aec" />

